{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Analysis in Python\n",
    "\n",
    "This tutorial provides an overview of common data anlysis tasks and packages in Python. \"Intro to Programming in Python\" is a pre-requisite, or working knowledge of data types and structures, loops and conditional statements, functions, reading and writing data. \n",
    "In this tutorial we will: \n",
    "- Import the packages needed. \n",
    "- Read in the data we're using as an example. \n",
    "- Clean and proces the data using different functions in the Pandas package. \n",
    "- Compute descriptive statistics for the variables of interest. \n",
    "- Display frequency and proportions tables. \n",
    "- Calculate correlations. \n",
    "- Conduct hypothesis tests. \n",
    "- Conduct regression analysis and regression diagnostics checks. \n",
    "- Visualize relations in the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules\n",
    "\n",
    "Everything that we've done so far was based on functions from base Python. However, we will often need to import other packages which can handle more complex or specific tasks. For example, we may want to use a module that is able to better read and write csv data, such as the 'csv' module. To do that, we have to first import the module. For packages that are already installed, you can simply do that by typing 'import' and the name of the package. Many of the useful packages are already installed in Anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do you know which packages are installed? If you open Anaconda Prompt, and you type \"conda list\", it will list all installed package. You can do this in any terminal/command prompt.\n",
    "\n",
    "If a package is not installed, you can install it in the Anaconda Prompt with: `conda install csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data in different formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'csv' module is already installed in Anaconda, so we can go ahead and import it. Let's read the file in csv format, recode missing values as NA, and write it out as a new clean.csv. The 'csv' module is very useful for manipulating large files that contain long text fields.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"clean.csv\", \"w\") as outfile:\n",
    "    writer=csv.writer(outfile)\n",
    "    with open(\"mydataset.csv\", \"r\") as infile:  # open the file for writing\n",
    "        reader=csv.reader(infile)\n",
    "        writer.writerow(next(reader))\n",
    "        for row in reader:\n",
    "            writer.writerow(row[0:6]+[(row[6].replace(\"missing\", \"NaN\"))])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also read csv files, as well as other file formats using Pandas. Pandas is one of the main libraries for data analysis in Python. For those of you familiar with R, the data frames structure and Pandas will make it very easy to use. Let's see what we can do, by importing the clean.csv file that you just saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # we import it as pd because it's easier to type\n",
    "df=pd.read_csv(\"clean.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5) # first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(3) # last 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # how many rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reelected\"][0:5] # select a column, and a slice within it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.region.unique() # Unique values in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting data: create another data frame that only includes obswervations from the South and East. \n",
    "value_list=[\"South\", \"East\"]\n",
    "df_SE=df[df.region.isin(value_list)] # Replace this with df[~df.region...] to keep only those that don't meet the condition\n",
    "df_SE.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only dataframes that meet multiple conditions:\n",
    "df_restricted=df[(df['region']==\"South\") & (df[\"chamber\"]==\"S\") & (df[\"reelected\"]==0)]\n",
    "df_restricted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group and aggregate \n",
    "grouped=df.groupby([\"region\", \"chamber\"])\n",
    "aggregated=grouped.agg({\"spent\":['sum','mean', 'min'], \n",
    "                       'raised':['sum', 'mean', 'max']})\n",
    "aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics\n",
    "\n",
    "Mean, median, 25th and 75th quartiles, min, max, number of missing observations, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For numeric variables in the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's turn off scientific notation:\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all variables in the dataset\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compute summary statistics on a single variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spent'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate one value of interest at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The sum of all the money raised\n",
    "df['raised'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cumulative sum of the money raised \n",
    "df['raised'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of non-NA values\n",
    "df['reelected'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minimum spending\n",
    "df['spent'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum spending\n",
    "df['spent'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean spending\n",
    "df['spent'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Median spending\n",
    "df['spent'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skewness of spending values\n",
    "df['spent'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample variance of spending\n",
    "df['spent'].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample standard deviation of spending\n",
    "df['spent'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kurtosis of spending values\n",
    "df['spent'].kurt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency and proportions tables\n",
    "You can make al types of frequency tables in Pandas using the *crosstab* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way table of frequencies\n",
    "pd.crosstab(index=df[\"chamber\"], columns=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way table of proportions\n",
    "pd.crosstab(index=df[\"chamber\"],  # Make a crosstab\n",
    "            columns=\"count\",  # Name the count column\n",
    "           normalize=\"all\")  # Display as percentages. Options are all, index(row), column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two way table of frequencies\n",
    "pd.crosstab(df.chamber, df.reelected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two way table of frequencies\n",
    "pd.crosstab(df.chamber, df.reelected, normalize=\"all\") #proportion of total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two way table of frequencies\n",
    "pd.crosstab(df.chamber, df.reelected, normalize=\"index\") #proportion of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two way table of frequencies\n",
    "pd.crosstab(df.chamber, df.reelected, normalize=\"columns\") #proportion of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two way table of frequencies\n",
    "pd.crosstab(df.chamber, df.reelected, normalize=\"all\", margins=True) # add row and column totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the rows and columns\n",
    "mytab = pd.crosstab(df.chamber, df.reelected, normalize=\"all\", margins=True)   # Include row and column totals\n",
    "mytab.columns = [\"Reelect_0\",\"Reelect_1\",\"Reelect_2\",\"Reelect_3\",\"Reelect_4\",\"Reelect_5\",\"Row_total\"]\n",
    "mytab.index= [\"House\",\"Senate\",\"Col_total\"]\n",
    "\n",
    "mytab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "heat_plot=seaborn.heatmap((mytab), annot=True) \n",
    "heat_plot.get_figure().savefig(\"heatmap_output.png\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation and covariance \n",
    "You can compute correlations in Pandas using the *corr* and *cov* functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For only two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"spent\"].corr(df[\"raised\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"spent\"].cov(df[\"raised\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing\n",
    "For the next section we'll import the *scipy.stats* sub-module of *scipy*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test if the population mean of data is likely to be equal to a given value (if observations are drawn from a Gaussian distributions of given population mean) by using the scipy.stats.ttest_1samp()function. It returns the T statistic, and the p-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(df['spent'], 320000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the mean spending in the House and Senate different, and it the difference statistically significant? We can do a 2-sample t-test with scipy.stats.ttest_ind():  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_spent = df[df['chamber'] == 'H']['spent']\n",
    "senate_spent = df[df['chamber'] == 'S']['spent']\n",
    "\n",
    "print(\"House mean:\", house_spent.mean(), \"Senate mean:\", senate_spent.mean(), stats.ttest_ind(house_spent, senate_spent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression analysis\n",
    "We can use the *ols* function from the *statsmodels* module to conduct OLS regression analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ols(\"spent ~ raised\", df).fit()\n",
    "print(model1.summary())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saveing the models to txt or csv: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model1_summary.txt', 'w') as f:\n",
    "    f.write(model1.summary().as_text())\n",
    "\n",
    "with open('model1_summary.csv', 'w') as f:\n",
    "    f.write(model1.summary().as_csv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression with dummy variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ols(\"spent ~ chamber\", df).fit()      \n",
    "print(model2.summary())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = ols(\"spent ~ region\", df).fit()      \n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = ols(\"spent ~ raised+chamber\", df).fit()\n",
    "print(model4.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression diagnostics. \n",
    "You can compute regression diagnostics using different funcions in *statsmodels*. You can read more about these here: [http://www.statsmodels.org/dev/diagnostic.html](http://www.statsmodels.org/dev/diagnostic.html). For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cook's distance\n",
    "import statsmodels.graphics.regressionplots as regplots\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = regplots.influence_plot(model4, ax=ax, criterion=\"cooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = statsmodels.graphics.regressionplots.plot_partregress_grid(model4, fig=fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Plot the dependent variable and fitted values with confidence intervals vs. an independent variable, the residuals of the model vs. the chosen independent variable, a partial regression plot, and a CCPR plot. Use function to quickly checking modeling assumptions with respect to a single regressor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = statsmodels.graphics.regressionplots.plot_regress_exog(model4, \"raised\", fig=fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitted values vs a chosen independent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "fig = statsmodels.graphics.regressionplots.plot_fit(model4, \"raised\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "What is the probability that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "from pandas.core import datetools\n",
    "model5 = glm(\"chamber~ raised+spent+reelected\", df, family=sm.families.Binomial()).fit()\n",
    "print(model5.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "To display graphs inline in Jupyter notebooks make sure you add \"%matplotlib inline\" in the first cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Histograms, comparing two distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_money=df[[\"raised\",\"spent\"]]\n",
    "df_money.plot.hist(stacked=True, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Barplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg2=grouped.agg({\"spent\":\"mean\", \n",
    "                  \"raised\":\"mean\"})\n",
    "agg2.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot with linear fit line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=df_money.raised.values\n",
    "y=df_money.spent.values\n",
    "fig, ax = plt.subplots()\n",
    "fit = np.polyfit(x, y, deg=1)\n",
    "ax.plot(x, fit[0] * x + fit[1], color='red')\n",
    "ax.scatter(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using Seaborn:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.regplot(x=\"raised\", y=\"spent\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.lmplot(x=\"raised\", y=\"spent\", hue=\"chamber\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore multiple regression relations with the module *seaborn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.pairplot(df, vars=['spent', 'raised'], kind='reg', hue=\"chamber\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "\n",
    "[Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "\n",
    "[Statsmodels documentation](http://www.statsmodels.org/stable/index.html)\n",
    "\n",
    "[Seaborn examples](https://seaborn.pydata.org/examples/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
