{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to text analysis in Python\n",
    "\n",
    "This section will provide a short overview of text analysis in Python. For an in-depth tretment of the topic please attend the workshop on Friday. The NLTK examples are based on \"Natural Language Processing with Python\" Bird, Klein and Loper, 2010. We will first have to install NLTK in Anaconda prompt and then import it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strings\n",
    "monty = \"Monty Python's \" \\\n",
    "         \"Flying Circus. \"\n",
    "print(monty*2 + \" plus just the last word: \" + monty[-8:])\n",
    "print(monty.find('Python')) #finds position of substring within string\n",
    "print(monty.upper() +' and '+ monty.lower())\n",
    "print(monty.replace('y', 'x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining and splitting strings to/from lists\n",
    "' '.join(['Monty', 'Python'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Monty Python'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular exppressions \n",
    "import re\n",
    "word = 'supercalifragilisticexpialidocious'\n",
    "#Example: find and count all vowels.  \n",
    "len(re.findall(r'[aeiou]', word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common cleaning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=\"The British people are telling us to “just get on with it”. Really? Where’s your evidence for that? \\\n",
    "Or is it wishful thinking? Coz I’m seeing Revoke, Revoke, Revoke - largest petition in history, in street stalls... \\\n",
    "and as the most popular options in the polls. #Brexit \\\n",
    "To those who are pretending that the withdrawal agreement is Brexit, it is clear you are either mistaken, \\\n",
    "deluded or dishonest.\\n\\nWe need to build trust in British politics.\\n\\n The WA will do no such thing. \\\n",
    "Latest CER estimate: the UK economy is 2.5 per cent smaller than it would be if Britain had voted remain. \\\n",
    "The knock-on hit to the public finances is £19 billion per annum – or £360 million a week.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#download data for the examples\n",
    "nltk.download() #please mind the window that opens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "#Tokenizing - divide into tokens\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing - turn to lower case\n",
    "lc_tokens=[w.lower() for w in tokens]\n",
    "lc_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords and keeping only alphanumerics\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "content = [w for w in lc_tokens if w.lower() not in stop_words and w.isalnum()]\n",
    "content[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "porter_stemmed=[porter.stem(t) for t in content]\n",
    "porter_stemmed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming - strip off affixes\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "lan_stemmed=[lancaster.stem(t) for t in content]\n",
    "lan_stemmed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "# Lemmatizing - the word is from a dictionary\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lematized=[wnl.lemmatize(t) for t in content]\n",
    "lematized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence segmentation\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sents = sent_tokenizer.tokenize(raw)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating on every element. List comprehension.  \n",
    "for word in tokens[0:5]:\n",
    "    if len(word)<=5 and word.endswith('e'):\n",
    "        print(word, ' is short and ends with e')\n",
    "    elif word.istitle():\n",
    "        print(word, ' is a titlecase word')\n",
    "    else:\n",
    "        print(word, 'is just another word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore how the words America and citizen are used in presidential inaugural speeches over time.\n",
    "import nltk\n",
    "nltk.download('inaugural')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (target, fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "    for w in inaugural.words(fileid)\n",
    "    for target in ['america', 'war']\n",
    "    if w.lower().startswith(target))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "verbs=[\"should\", \"may\", \"can\"]\n",
    "genres=[\"news\", \"government\", \"romance\"]\n",
    "for g in genres:\n",
    "    words=brown.words(categories=g)\n",
    "    freq=FreqDist([w.lower() for w in words if w.lower() in verbs])\n",
    "    print(g, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "#finding words that characterize a text, relatively long, and occur frequently\n",
    "fdist = FreqDist(text4)\n",
    "sorted([w for w in set(text4) if len(w) > 5 and fdist[w] > 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how often a word occurs in a text,\n",
    "text4.count(\"democracy\")\n",
    "# compute what percentage of the text is taken up by a specific word\n",
    "100 * text4.count('democracy') / len(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of a word in the text: how many spaces from the beginning does it appear? \n",
    "#This positional information can be displayed using a dispersion plot. \n",
    "#You need NumPy and Matplotlib.\n",
    "nltk.download('treebank')\n",
    "from nltk.book import text4\n",
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"war\", \"America\", \"vote\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words in context\n",
    "text4.concordance(\"vote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What other words appear in a similar range of contexts? \n",
    "text4.similar(\"vote\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing collected tweets\n",
    "Next, we need to process the json file, extract relevant fields and put them in a database for coding. The file is very large, so we won't process it live because it would take to long. Let's have a look at what it looks like and identify the relevant fields.\n",
    "\n",
    "### Things to consider:\n",
    "\n",
    "* Which fields are the relevant ones? \n",
    "\n",
    "* How do we make sure we have the full text of the tweet? \n",
    "\n",
    "* What should we do about retweets?\n",
    "\n",
    "* How do we remove extra white spaces from the text? \n",
    "\n",
    "* What other cleaning do you think we'd need to do? \n",
    "\n",
    "* If we wanted to code some tweets, how do we draw a sub-sample of tweets for coding? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "#total_tweets=0\n",
    "#retweets=0\n",
    "#with open('brexit_tweets_selected_fields_a.csv', 'w') as outfile:    \n",
    "#    writer = csv.writer(outfile, delimiter=',', lineterminator='\\n')\n",
    "#    header = ['tweet_id', 'user_id', 'screen_name', 'followers_count', 'friends_count', 'statuses_count', 'tweet_date', 'retweeted_status_id', 'retweet_count', 'text']  \n",
    "#    writer.writerow(header)    \n",
    "#    with open(\"brexit_tweets.json\", \"rb\") as json_file:\n",
    "#        for line in json_file:\n",
    "#            i=json.loads(line.decode('utf8'))\n",
    "#            #total_tweets=total_tweets+1\n",
    "#            date=str(i[\"created_at\"])\n",
    "#            try:\n",
    "#                rt_id=i[\"retweeted_status\"][\"id_str\"]\n",
    "#                text=i[\"retweeted_status\"][\"full_text\"].replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').strip()\n",
    "#                #text_str=text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').strip()\n",
    "#                retweets=retweets+1\n",
    "#                total_tweets=total_tweets+1\n",
    "#            except:\n",
    "#                rt_id=\"none\"\n",
    "#                text=i[\"full_text\"].replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').strip()\n",
    "#                total_tweets=total_tweets+1\n",
    "#                #text_str=text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').strip()\n",
    "#            #print(text)\n",
    "#            writer.writerow([i[\"id\"]]+[i[\"user\"][\"id\"]]+[i[\"user\"][\"screen_name\"]]+[i[\"user\"][\"followers_count\"]]+[i[\"user\"][\"friends_count\"]]+[i[\"user\"][\"statuses_count\"]]+[date]+[rt_id]+[i[\"retweet_count\"]]+[text.encode('utf-8')])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
