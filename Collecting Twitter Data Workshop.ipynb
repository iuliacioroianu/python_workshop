{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tutorial contents\n",
    "* [Different ways of collecting Twitter data](#Different-ways-of-collecting-Twitter-data)\n",
    "* [Providing authorization to the Twitter API](#Providing-authorization-to-the-Twitter-API)\n",
    "* [Collecting tweets](#Collecting-tweets)\n",
    "* [Getting information about a user account](#Getting-information-about-a-user-account)\n",
    "* [Getting follower IDs](#Getting-follower-IDs) \n",
    "* [Getting the IDs of users being followed by a specified account](#Getting-the-IDs-of-users-being-followed-by a-specified-account) \n",
    "* [Getting tweets favorited by a user](#Getting-tweets-favorited-by-a-user)\n",
    "* [Getting info on friendship relations](#Getting-info-on-friendship-relations)\n",
    "* [Getting retweets of a certain status](#Getting-retweets-of-a-certain-status)\n",
    "* [Searching for tweets](#Searching-for-tweets)\n",
    "* [Rate limits and cursor](#Rate-limits-and-cursor)\n",
    "* [Getting Brexit tweets](#Getting-Brexit-tweets)\n",
    "* [Data processing](#Data-processing)\n",
    "* [Designing crowdsourcing job](#Designing-crowdsourcing-job)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways of collecting Twitter data\n",
    "![collection_methods](collecting_tweets_options.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter APIs\n",
    "API stands for Application Programming Interface. They allow developers to build tools and applications based on the data stored. And they give researchers interested in data collection an easy way to access the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing authorization to the Twitter API\n",
    "\n",
    "The first step is to become a Twitter developer. For this you need a Twitter account yourself, and [to create a new app](https://apps.twitter.com/).\n",
    "\n",
    "\n",
    "You need a unique name. You can fill in anything as the website and description. \n",
    "![create_app](create_app.png)\n",
    "\n",
    "Once you're a developer, you will found your access credentials under the Keys and Access Tokens tab of your new app. You will need to copy the following fields in this form:\n",
    "\n",
    "1. Consumer Key (API Key)\n",
    "2. Consumer Secret (API Secret)\n",
    "3. Access Token\n",
    "4. Access Token Secret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the consumer key and secret: \n",
    "![consumer_key](consumer_key.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the access token and token secret: \n",
    "![token_action](token_actions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![access_token](access_token.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we have access. \n",
    "\n",
    "The next thing we need to do it install Tweepy. Tweepy is a Python library for accessing the Twitter API. You can install it by typing: \n",
    "\n",
    "    pip install tweepy\n",
    "\n",
    "in your Anaconda Prompt. \n",
    "    \n",
    "This provides a convenient front-end for the Twitter API, giving us easy access without having to venture outside of our Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "CONSUMER_KEY        = 'vRoZs3Z27vyb7FFAmNl2EJ7Ei'\n",
    "CONSUMER_KEY_SECRET = 'RLcA5LBkaZHgTFzGTuiOXn4SEgoLh62HFw7gmYcsfoWOTgNgmX'\n",
    "ACCESS_TOKEN        = '1904917267-5J95nCkfnY8E8XMdNZeDKk0wpIbdo767TunsCAw'\n",
    "ACCESS_TOKEN_SECRET = '9YtZyqN0rZrcPkOokDZPRw7VwOQXl9nQbtUukVXBQiU8I'\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_KEY_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "print(api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have access to the Twitter API, there are a range of different requests we can make. We can use GET to retreive information about any public users or tweets, and even POST to make changes to the account we used to authorize, such as following accounts and making tweets. All functions of the API are [thoroughly documented](https://dev.twitter.com/rest/reference), so below we will only go over a few examples of the most common tasks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting tweets\n",
    "\n",
    "Statuses posted by a specified user can be collected with a [GET statuses/user_timeline](https://dev.twitter.com/rest/reference/get/statuses/user_timeline) request. We need to specify either the ID, user ID or screen name of the user, and we can include other options such as the number of statuses to retrieve, the first and last status to be collected, and whether retweets should be included or not. If provided, `count` limits the number of results returned from the search. Otherwise you will simply encounter the rate limit on the Twitter API or the end of the user's timeline. Retweets are counted towards your app rate limit. See the last section on [Rate limits and cursor](#Rate-limits-and-cursor) to learn how to handle rate limits and get more tweets using the cursor. \n",
    "\n",
    "The search returns a list of *Status* objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = api.user_timeline(screen_name = 'theresa_may', count = 100, include_rts = True) \n",
    "search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `Status` object contains a number of relevant fields, which can be accessed with `status.[field_name]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status=search[0]\n",
    "print(\"Tweet text:\", status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a list of all the field names:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in status.__dict__.items():  #same thing as `vars(status)`\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the entire content of the request is not very informative, since it contains a large amount of meta-data. And while it is useful to know how to access particular fields, often what we want is to retrieve all the information and store it somewhere for later processing. We will therefore write our search output to a file, where each line corresponds to a tweet in .json format. Note that this is one of the fields included in the `Status` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First change the working directory to the location where you downloaded the files for today\n",
    "import os\n",
    "os.chdir(\"C:/Code/teaching\")\n",
    "import json\n",
    "F_NAME = 'theresa_may_timeline.json'\n",
    "with open(F_NAME,'w') as f_out:\n",
    "    for status in search:\n",
    "        json.dump(status._json, f_out)\n",
    "        f_out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can open this file in notepad to investigate it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting information about a user account\n",
    "\n",
    "We can also get detailed information about an account, such as the account description, number of followers, number of users followed, the date the account was created, location, number of tweets, a link to the profile image, number of favorites, etc. The argument needed is either `id`, `user_id` or `screen_name`. The output is a User object. We will again save the output object as a .json file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info = api.get_user(screen_name = 'theresa_may')\n",
    "print(\"Account description:\", user_info.description)\n",
    "print(\"Following:\", user_info.friends_count)\n",
    "print(\"Followers:\", user_info.followers_count)\n",
    "\n",
    "#for key,value in user_info.__dict__.items():  #same thing as `vars(status)`\n",
    "#    print key\n",
    "\n",
    "F_NAME = 'theresa_may_user_info.json'\n",
    "with open(F_NAME,'w') as f_out:\n",
    "    json.dump(user_info._json, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting follower IDs\n",
    "\n",
    "We can get a list of the IDs of the first 5000 users following a certain account with `api.followers([id/screen_name/user_id])`. See the [Rate limits and cursor](#Rate-limits-and-cursor) section at the end to find out how to get more than the first 100 users.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers = api.followers_ids(screen_name = 'theresa_may', count=100)\n",
    "\n",
    "#Save list of followers:\n",
    "F_NAME = 'theresa_may_followers.txt'\n",
    "with open(F_NAME,'w') as f_out:\n",
    "    for follower in followers:\n",
    "        f_out.write(\"%s\\n\" % follower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the IDs of users being followed by a specified account\n",
    "\n",
    "We can also get the IDs of users being followed by the specified user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends = api.friends_ids(screen_name = 'theresa_may')\n",
    "print(\"May follows\", len(friends), \"users.\")\n",
    "\n",
    "F_NAME = 'followed_by_theresa_may.txt'\n",
    "with open(F_NAME,'w') as f_out:\n",
    "    for friend in friends:\n",
    "        f_out.write(\"%s\\n\" % friend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting tweets favorited by a user\n",
    "We can get a list of tweets favorited by a user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorites = api.favorites(screen_name = 'theresa_may')\n",
    "print(\"Number of likes:\", len(favorites))\n",
    "\n",
    "F_NAME = 'theresa_may_favorites.json'\n",
    "with open(F_NAME,'w') as f_out:\n",
    "    for favorite in favorites:\n",
    "        json.dump(favorite._json, f_out)\n",
    "        f_out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting info on friendship relations\n",
    "\n",
    "We can get informaton about the existance of a friendhip between two users (a `subject user` and a `target`), and other characeristics of the relation with `api.show_friendship(source_id/source_screen_name, target_id/target_screen_name)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friendship=api.show_friendship(source_screen_name=\"theresa_may\", target_screen_name=\"ExeterQStep\")\n",
    "print(\"Source(May) followed by target(Exeter Q-Step)?\", friendship[0].followed_by)\n",
    "print(\"Target(Exeter Q-Step) followed by source(May)?\", friendship[1].followed_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting retweets of a certain status\n",
    "`api.retweets(id[, count])` returns up to 100 of the first retweets of a given tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets = api.retweets(id = 701057384869969921, count=100)\n",
    "retweets\n",
    "F_NAME = 'status_retweets.json'\n",
    "with open(F_NAME,'w') as f_out:\n",
    "    for retweet in retweets:\n",
    "        json.dump(retweet._json, f_out)\n",
    "        f_out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Tweets\n",
    "Twitter offers different options for searches. \n",
    "\n",
    "![search_options](search_options.png)\n",
    "\n",
    "\"Standard\" means free (and incomplete), and this is what we get unless we contact Twitter/Gnip sales and get a paid account. \n",
    "Let's find 10 tweets that mention \"Brexit\" and \"NHS\" and see what the first one in the returned list looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Brexit AND NHS\"\n",
    "max_tweets = 10\n",
    "searched_tweets = [status for status in tweepy.Cursor(api.search, q=query).items(max_tweets)]\n",
    "print([status.text for status in searched_tweets[0:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say we want to restrict our search to tweets that are in English, which are from the UK. We also want to get up to 100 tweets, and write the results out to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweets=10\n",
    "searched_tweets = [status for status in tweepy.Cursor(api.search, q=query, \n",
    "                                                      geocode=\"54.323486,-3.396256,500km\", \n",
    "                                                      lang=\"en\").items(max_tweets)]\n",
    "F_NAME = 'Search_tweets.json'\n",
    "with open(F_NAME,'w') as f_out:\n",
    "    for status in searched_tweets:  \n",
    "        json.dump(status._json, f_out)\n",
    "        f_out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rate limits and cursor\n",
    "\n",
    "Twitter [API rate limits](https://dev.twitter.com/rest/public/rate-limiting) are limiting the number of requests you can make in a certain time frame. Tweepy can help handle these limitations. \n",
    "First, you can set a number of additional parameters in the `tweepy.api` class: \n",
    "* `retry_count` – default number of retries to attempt when error occurs\n",
    "* `retry_delay` – number of seconds to wait between retries\n",
    "* `retry_errors` – which HTTP status codes to retry\n",
    "* `wait_on_rate_limit` – Whether or not to automatically wait for rate limits to replenish\n",
    "* `wait_on_rate_limit_notify` – Whether or not to print a notification when Tweepy\n",
    "Setting the last two parameters to `True` usually handles the rate limits. \n",
    "So we can redefine our API instance with these parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth, \n",
    "                 retry_count=5,\n",
    "                 retry_delay=10,\n",
    "                 retry_errors=set([401, 404, 500, 503, 429]),\n",
    "                 wait_on_rate_limit=True,\n",
    "                 wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle pagination, Tweepy has the extremely helpful Cursor object. Instead of manually iterating through the pages of a user timeline, we can use the cursor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_NAME = 'ExeterQStep_timeline_all.json'\n",
    "with open(F_NAME,'w') as f_out:\n",
    "    for status in search:\n",
    "        for status in tweepy.Cursor(api.user_timeline, screen_name = 'ExeterQStep', include_rts = True).items():\n",
    "            json.dump(status._json, f_out)\n",
    "            f_out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is going to take even longer, so don't run it now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "row"
   },
   "outputs": [],
   "source": [
    "#F_NAME = 'ExeterQStep_followers.txt'            \n",
    "#Save list of followers:\n",
    "#with open(F_NAME,'w') as f_out:\n",
    "#    for follower in tweepy.Cursor(api.followers_ids, screen_name = 'theresa_may').items():\n",
    "#        f_out.write(\"%s\\n\" % follower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Brexit tweets to analyse sentiment\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Your task is to analyse the sentiment in recent tweets about Brexit. Think about how you might do that, what type of data you need and what the first steps in the analysis should be. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by collecting the most recent tweets that the Standard search API gives us access to. The code below is uncommented because the full collection takes a few hours. Uncomment and edit it if you want to run your own twitter search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"brexit_tweets_april_1.json\",  \"w\", encoding=\"utf-8\") as f_out:\n",
    "#    for tweets in tweepy.Cursor(api1.search,\n",
    "#                                q='brexit or BREXIT or Brexit',\n",
    "#                                tweet_mode=\"extended\",\n",
    "#                                since=\"2019-03-31\", \n",
    "#                                until=\"2018-04-01\",\n",
    "#                                lang=\"en\",\n",
    "#                                result_type='recent',\n",
    "#                                geocode=\"54.323486,-3.396256,500km\",\n",
    "#                                include_entities=True,\n",
    "#                                monitor_rate_limit=True, \n",
    "#                                wait_on_rate_limit=True).items():\n",
    "#        # convert from Python dict to JSON: \n",
    "#        data = json.dumps(tweets._json, ensure_ascii=False)\n",
    "#        f_out.write(data)\n",
    "#        f_out.write('\\n')\n",
    "#        count=count+1\n",
    "#        if count % 1000 == 0:\n",
    "#            print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra resources on Twitter data collection\n",
    "\n",
    "Twitter developer resources and documentation, including tutorials: \n",
    "https://developer.twitter.com/en/docs\n",
    "\n",
    "This list of all API methods is particularly useful:\n",
    "https://developer.twitter.com/en/docs/api-reference-index\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
