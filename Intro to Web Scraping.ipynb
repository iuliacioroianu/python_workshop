{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial contents\n",
    "**Part 1: Introduction**\n",
    "* [What is web scraping?](#What-is-web-scraping?)\n",
    "* [Libraries overview](#Libraries)\n",
    "\n",
    "**Part 2: Basic web scraping**\n",
    "* [Making web requests](#Making-web-requests)\n",
    "* [Intro to HTML](#Intro-to-HTML)\n",
    "* [Parsing HTML, selecting and extracting relevant info](#Parsing-HTML)\n",
    "* [Building a database out of the scraped data](#Saving-data)\n",
    "\n",
    "**Part 3: Advanced topics**\n",
    "* [Browser emmulation](#Browser-emmulation)\n",
    "\n",
    "**Part 4: Other advice**\n",
    "* [Legal and ethical issues](#Legal-and-ethical-concerns)\n",
    "* [Boxed solutions - Boierplate and other methods](#Boxed-solutions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Introduction\n",
    "## What is web scraping?\n",
    "Collecting information from web pages. There are multiple ways of doing this, such as writing your own scraper, using APIs, using boxed software and tools. Today we will focus on the first type, but I will provide resources that cover the other two broad categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "Multiple useful libraries. The main ones that we'll be using today are [Requests](http://docs.python-requests.org/en/master/), [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/), and [Selenium WebDriver](http://selenium-python.readthedocs.io/). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Basic web scraping\n",
    "## Making web requests\n",
    "When you open a web page, you send instructions to a web server, requesting for the information to be displayed in your browser. The server in return sends the code (usually HTML) to your browser. The browser translates the code using a Document Object Model(DOM). \n",
    "\n",
    "\n",
    "Let's make a request for the IOC webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the url\n",
    "url = \"https://www.iocsummerschoolexeter.org/\"\n",
    "# Query the website and return the html. \n",
    "# Save the html as a new variable page.  \n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the content of the page object, which is what we'll be working on later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content[0:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to HTML\n",
    "The response looks intimidating, but once you understand basic HTTP things look a lot easier. Let's have a look at what this code would look like for a very simple page. Go to www.example.com, and right click anywhere in the page, selecting \"View page source\". \n",
    "You need to be aware of the simple elements of syntax for HTML pages. \n",
    "Every tag serves a block inside the webpage:\n",
    "1. <!DOCTYPE html>: HTML documents must start with a type declaration.\n",
    "2. The HTML document is contained between html and /html.\n",
    "3. The meta and script declaration of the HTML document is between head and /head.\n",
    "4. The visible part of the HTML document is between body and /body tags.\n",
    "5. Title headings are defined with the h1 through hn tags.\n",
    "6. Paragraphs are defined with the p tag.\n",
    "\n",
    "Other useful tags include a for hyperlinks, table for tables, tr for table rows, and td for table columns.\n",
    "\n",
    "HTML tags sometimes come with id or class attributes. The id attribute specifies a unique id for an HTML tag and the value must be unique within the HTML document. The class attribute is used to define equal styles for HTML tags with the same class. We can make use of these ids and classes to help us locate the data we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML\n",
    "The next step is to parse the HTTP and get the info that is relevant to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the html in the 'page' variable, and store it in Beautiful Soup format\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the page title\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string\n",
    "# Returns the content inside a title tag as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a\n",
    "# Return the first matching anchor tag</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, we could use the find all, and return all the matching anchor tags\n",
    "soup.find_all('a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or to to get them in a nicer format: \n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or to to get only links that contain a certain text\n",
    "import re\n",
    "for link in soup.find_all('a', href=re.compile(\"mailto\")):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Advanced topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browser emmulation\n",
    "Sometimes browsers make it hard (or impossible) for you to scrape them using these methods. \n",
    "\n",
    "\n",
    "Sometimes you have to pretend that you are human! \n",
    "Selenium allows you to do that. \n",
    "\n",
    "Let's first download [Cromedriver](https://chromedriver.storage.googleapis.com/index.html?path=2.40/), save it an unzip it to a location of your choice. You will have to remember that location and replace it in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "#####################################\n",
    "# Setup\n",
    "#####################################\n",
    "chromedriver='C:/ProgramData/Anaconda3/chromedriver.exe'\n",
    "# Load browser\n",
    "browser = webdriver.Chrome(executable_path=chromedriver)\n",
    "# Got to UoE library page to access the Q-Step page\n",
    "url = 'http://socialsciences.exeter.ac.uk/q-step/'\n",
    "browser.get(url)\n",
    "time.sleep(random.uniform(3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to XPaths\n",
    "XPaths provide a useful way to navigate elements in a DOM hierarchy. They provide the \\path\" (or address) to particular objects in the DOM. Some useful features include the ability to select object be id, select all objects of a specific type, extracting text, select all elements that contain a certain object. Let's have a look at the page source for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some ofher sections\n",
    "browser.find_element_by_xpath('//*[@id=\"exeter-nav\"]/li[3]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand an element on the page\n",
    "browser.find_element_by_xpath('//*[@id=\"ac0\"]/div[2]/div[1]/h4/a').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to save all the names, positions and departments of academic staff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table object\n",
    "table = browser.find_element_by_xpath('//*[@id=\"ac0a1\"]/div/table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the table column names\n",
    "col_names=[]\n",
    "head=browser.find_element_by_xpath('//*[@id=\"ac0a1\"]/div/table/tbody/tr[1]')\n",
    "data=head.find_elements_by_tag_name('th')\n",
    "for name in data:\n",
    "    col_names.append(name.text)\n",
    "    #col_name=name.text.encode('utf8')\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's store all the info into a Pandas data frame. \n",
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's get the row info \n",
    "body = table.find_element_by_tag_name('tbody')\n",
    "file_data = []\n",
    "body_rows = body.find_elements_by_tag_name('tr')\n",
    "for row in body_rows[1:]:\n",
    "    data = row.find_elements_by_tag_name('td')\n",
    "    file_row = []\n",
    "    for dat in data:\n",
    "        dat_text = dat.text\n",
    "        file_row.append(dat_text)\n",
    "    file_data.append(file_row)\n",
    "df = df.append(pd.DataFrame(file_data, columns=col_names),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data\n",
    "Saving the table we just created to a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"staff_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (20 minutes): \n",
    "1. Extract the names, positions and departments of ALL members of staff and save them to a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can select buttons, elements in drop down menus, and insert text in fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to contact page\n",
    "browser.find_element_by_xpath('//*[@id=\"exeter-nav\"]/li[11]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the find an academic button\n",
    "browser.find_element_by_xpath('//*[@id=\"searchoption2\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write query\n",
    "text_field=browser.find_element_by_xpath('//*[@id=\"query\"]')\n",
    "text_field.send_keys(\"banducci\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click send\n",
    "browser.find_element_by_xpath('//*[@id=\"right-contents\"]/form/div/div/button').click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back\n",
    "browser.back()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: Further advice\n",
    "## Legal and ethical concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules of conduct:\n",
    "1. Respect the site's robot.txt. You should check a websiteâ€™s Terms and Conditions before you scrape it. Be careful to read the statements about legal use of data. Usually, the data you scrape should not be used for commercial purposes.\n",
    "2. Be kind to the servers, do not overburden them, build delays into your scripts. Do not request data from the website too aggressively with your program (also known as spamming), as this may break the website. Make sure your program behaves in a reasonable manner (i.e. acts like a human). One request for one webpage per second is good practice.\n",
    "3. Make sure you follow university institutional review board and ethics eommittee guidelines. \n",
    "4. Follow the laws, rules and regulations related to privacy, data use, property rights, etc. \n",
    "5. The layout of a website may change from time to time, so make sure to revisit the site and rewrite your code as needed"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
